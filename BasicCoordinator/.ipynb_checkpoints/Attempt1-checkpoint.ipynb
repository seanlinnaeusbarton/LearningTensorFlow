{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import numpy as np\n",
    "import pyMCCM as ccm\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        #List out our bandits. Currently arms 4, 2, and 1 (respectively) are the most optimal.\n",
    "        self.bandits = np.array([[0.2,0,-0.0,-5],[0.1,-5,1,0.25],[-5,5,5,5]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "        \n",
    "    def getBandit(self):\n",
    "        self.state_onehot = np.zeros(len(self.bandits))\n",
    "        self.state = np.random.randint(0,len(self.bandits)) #Returns a random state for each episode.\n",
    "        self.state_onehot[self.state] = 1\n",
    "        return self.state\n",
    "        \n",
    "    def pullArm(self,action):\n",
    "        #Get a random number.\n",
    "        bandit = self.bandits[self.state,action]\n",
    "        result = np.random.randn(1)\n",
    "        if result > bandit:\n",
    "            #return a positive reward.\n",
    "            return 1\n",
    "        else:\n",
    "            #return a negative reward.\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self,max_size):\n",
    "        self.buffer = deque([])\n",
    "        self.current = []\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.buffer))\n",
    "    \n",
    "    def add(self,state,t):\n",
    "        if t == 0:\n",
    "            if len(self.buffer) >= self.max_size:\n",
    "                self.buffer.popleft()\n",
    "            self.buffer.append(self.current)\n",
    "            self.current = []\n",
    "        self.current.append(state)\n",
    "    \n",
    "    def recall(self,batch):\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,ID=0,observation_space,action_space,observation_type=tf.int32,action_type=tf.int32):\n",
    "        self.lr = 0.95 # learning rate\n",
    "        self.memory_size = 1e4\n",
    "        self.batch_size = 20\n",
    "        self.ID = ID\n",
    "        \n",
    "        self._obs_space = observation_space\n",
    "        self._act_space = action_space\n",
    "        self.inputs = tf.placeholder(shape=[None, self._obs_space], dtype=observation_type)\n",
    "        self.outputs = _make_network(scope=\"Agent{}\".format(self.ID))\n",
    "        self.action = tf.argmax(self.outputs)\n",
    "        \n",
    "        self.memory = Memory()\n",
    "        \n",
    "    def _make_network(self,layer_nodes=16,activation=tf.nn.relu,scope,reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            net = self.inputs\n",
    "            net = layers.fully_connected(net, num_outputs=layer_nodes, activation_fn=activation)\n",
    "            net = layers.fully_connected(net, num_outputs=layer_nodes, activation_fn=activation)\n",
    "            net = layers.fully_connected(net, num_outputs=outputs, activation=tf.nn.sigmoid)\n",
    "            return(net)\n",
    "    \n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.memory_size:\n",
    "            return\n",
    "    \n",
    "    def act(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "total_episodes = 10000\n",
    "episode_length = 25\n",
    "episode_rewards = []\n",
    "explore = 0.99\n",
    "\n",
    "init = tf.global_variable_initializer()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    for episode in range(total_episodes):\n",
    "        obs_0 = [agent.act]\n",
    "        for t in range(episode_length):\n",
    "            if explore < 0.01:\n",
    "                explore = 0.01\n",
    "                \n",
    "            obs = bandit.getBandit()\n",
    "            if np.random.rand(1) < explore:\n",
    "                action = np.random.randit(cBandit.num_actios)\n",
    "            else:\n",
    "                action = sess.run(agent.action, feed_dict{agent.inputs: [obs]})\n",
    "                \n",
    "            reward = bandit.pullArm(action)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
