{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import numpy as np\n",
    "#import pyMCCM as ccm\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit():\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        #List out our bandits. Currently arms 4, 2, and 1 (respectively) are the most optimal.\n",
    "        self.bandits = np.array([[0.2,0,-0.0,-5],[0.1,-5,1,0.25],[-5,5,5,5]])\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "        \n",
    "    def getBandit(self):\n",
    "        self.state_onehot = np.zeros(len(self.bandits))\n",
    "        self.state = np.random.randint(0,len(self.bandits)) #Returns a random state for each episode.\n",
    "        self.state_onehot[self.state] = 1\n",
    "        return self.state_onehot\n",
    "        \n",
    "    def pullArm(self,action):\n",
    "        #Get a random number.\n",
    "        bandit = self.bandits[self.state,action]\n",
    "        result = np.random.randn(1)\n",
    "        if result > bandit:\n",
    "            #return a positive reward.\n",
    "            return 1\n",
    "        else:\n",
    "            #return a negative reward.\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self,max_size):\n",
    "        self.buffer = deque([])\n",
    "        self.current = []\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.buffer))\n",
    "    \n",
    "    def add(self,state,t):\n",
    "        if t == 0:\n",
    "            if len(self.buffer) >= self.max_size:\n",
    "                self.buffer.popleft()\n",
    "            self.buffer.append(self.current)\n",
    "            self.current = []\n",
    "        self.current.append(state)\n",
    "    \n",
    "    def recall(self,batch):\n",
    "        idx = [i for i in range(len(self.buffer))]\n",
    "        np.random.shuffle(idx)\n",
    "        out = [self.buffer[x] for x in idx[0:batch]]\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,ID,observation_space,action_space,observation_type=tf.float32,action_type=tf.float32):\n",
    "        self.lr = 0.95 # learning rate\n",
    "        self.memory_size = 1e4\n",
    "        self.batch_size = 20\n",
    "        self.ID = ID\n",
    "        \n",
    "        self._obs_space = observation_space\n",
    "        self._act_space = action_space\n",
    "        self.inputs = tf.placeholder(shape=[None, self._obs_space], dtype=observation_type)\n",
    "        self.outputs = self._make_network(scope=\"Agent{}\".format(self.ID))\n",
    "        self.action = tf.argmax(self.outputs)\n",
    "        \n",
    "        self.memory = Memory(self.memory_size)\n",
    "        \n",
    "        self.responsible_weight = tf.slice(self.outputs,self.)\n",
    "        self.loss = -tf.log()\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        \n",
    "    def _make_network(self,scope,layer_nodes=16,activation=tf.nn.relu,reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            net = self.inputs\n",
    "            net = layers.fully_connected(net, num_outputs=layer_nodes, activation_fn=activation)\n",
    "            net = layers.fully_connected(net, num_outputs=layer_nodes, activation_fn=activation)\n",
    "            net = layers.fully_connected(net, num_outputs=self._act_space, activation_fn=tf.nn.sigmoid)\n",
    "            return(net)\n",
    "    \n",
    "    def experience(self,t,obs,act,rew,new_obs,done=None,terminal=None):\n",
    "        self.memory.add([obs,act,rew,new_obs,done,terminal],t)\n",
    "    \n",
    "    def learn(self):\n",
    "        #if len(self.memory) < self.memory_size:\n",
    "            #return\n",
    "        \n",
    "        replay = self.memory.recall(self.batch_size)\n",
    "        replay = [y for x in replay for y in x]\n",
    "        np.random.shuffle(replay)\n",
    "        return(replay)\n",
    "        \n",
    "        \n",
    "    def act(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    agent = Agent(0,3,2)\n",
    "\n",
    "    for i in range(100):\n",
    "        for j in range(25):\n",
    "            agent.experience(j,\\\n",
    "                            np.random.randint(3),\\\n",
    "                            np.random.randint(4),\\\n",
    "                            np.random.random(1)[0],\\\n",
    "                            np.random.randint(3))\n",
    "\n",
    "    print(agent.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "total_episodes = 10000\n",
    "episode_length = 25\n",
    "episode_rewards = []\n",
    "explore = 0.99\n",
    "num_agents = 1\n",
    "agents = []\n",
    "\n",
    "init = tf.global_variable_initializer()\n",
    "with tf.Session() as sess:\n",
    "    cBandit = Bandit()\n",
    "    \n",
    "    for a in range(num_agents):\n",
    "        agents.append(Agent(a,len(cBandit.bandits),len(cBandit.num_actions)))\n",
    "    \n",
    "    agent = agents[0]\n",
    "    for episode in range(total_episodes):\n",
    "        obs_0 = [agent.act]\n",
    "        for t in range(episode_length):\n",
    "            if explore < 0.01:\n",
    "                explore = 0.01\n",
    "                \n",
    "            obs = bandit.getBandit()\n",
    "            if np.random.rand(1) < explore:\n",
    "                action = np.random.randit(cBandit.num_actions)\n",
    "            else:\n",
    "                action = sess.run(agent.action, feed_dict{agent.inputs: [obs]})\n",
    "                \n",
    "            reward = bandit.pullArm(action)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
